---
title: "lab 5 2019 Q"
subtitle: "Survival analysis "
author: "Iris Ganser and Marshall Lloyd"
date: "Winter 2019"
output:
  html_document:
    css: lab.css
    highlight: tango
    theme: cerulean
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<div id="instructions">
Complete this exercise, and submit answers using R Markdown in RStudio. Open the document using RStudio, then **save as XXXXXXXXX.Rmd where XXXXXXXXX is your McGill student ID. Then when you knit the file it will create a html file called XXXXXXXXX.html**. You can then zip this file and submit it. Alternatively you can knit the file into a Word or PDF document and submit those. The zip file approach is best for maintianing all the formating. Unfortuantely, as discussed in the announcements MyCourses won't accept a html file without altering its formating. Enter your answers in the appropriate R code chunks or text answers in the box following the **Type your answer here:**.   

Grading is based on the points allocated for each question and an extra 5 points for overall style, organization, and clarity. Marks per question are indicated in each question. 

</div>

## Questions 1 - 14 points
We will use several datasets from the `asaur` library; load the `pancreatic` dataset from the `asaur` library. The command `help("pancreatic")` will explain what the variables are. For survival analysis obviously we need to load the `survival` package. To easily manipulate dates for the right format for survival analysis, the `lubridate` package is helpful.

<div id="exercise">
**Exercise**:     
a) How many observations and variables are in this dataset? (2 points)      
b) We are interested in the progression free survival (PFS) time. In some cases PFS is not available and can be approximated by overall survival (OS) time. What is the median PFS? What is the median OS? (6 points) 
c) Next, plot the survival curves according to the stage of the disease and report if there is evidence for a difference between these groups. Plot time axis in months. (4 points)          
d) Report the p-value to 3 decimal places. How do you interpret this? (2 points)
</div>

```{r}
# insert R code here


#install.packages("asaur")
#install.packages("lubridate")
library(asaur)
library(lubridate)
library(survival)

help("pancreatic")

q1.data <- pancreatic

#a) How many observations and variables are in this dataset? (2 points)   
head(pancreatic)
str(pancreatic)
str(q1.data)

#b) We are interested in the progression free survival (PFS) time. In some cases PFS is not available and can be approximated by overall survival (OS) time. What is the median PFS? What is the median OS? (6 points) 

q1.data$onstudy <- as_date(mdy(q1.data$onstudy))
q1.data$progression <- as_date(mdy(q1.data$progression))
q1.data$death <- as_date(mdy(q1.data$death))

head(q1.data)

q1.data$PFS <- q1.data$progression - q1.data$onstudy
q1.data$OS <- q1.data$death - q1.data$onstudy

median.PFS <- median(q1.data$PFS, na.rm = TRUE)
median.OS <- median(q1.data$OS)

q1.data$SurvMo <- time_length(interval(q1.data$onstudy, if_else(is.na(q1.data$PFS), q1.data$death, q1.data$progression)), unit = "month")

q1.surv.object <- Surv(q1.data$SurvMo)

#c) Next, plot the survival curves according to the stage of the disease and report if there is evidence for a difference between these groups. Plot time axis in months. (4 points)          
head(q1.data)

library(dplyr)
q1.data


#note that survfit() col is assigned in the order of the factors, ie: LA is 1, so that one is blue
plot(survfit(q1.surv.object ~ q1.data$stage), col = c("blue", "red"), xlab = "Survival Time (months)", ylab = "Survival Probability")
legend(x = "topright", inset = c(0.05, 0.05), col = c("red", "blue"), lty = c(1, 1), legend = c("Mestatic", "Locally Advanced"))
title(main = "Survival of Pancreatic Cancer by Stage")

q1.curvetest <- survdiff(q1.surv.object ~ q1.data$stage)
q1.curvetest$chisq


#d) Report the p-value to 3 decimal places. How do you interpret this? (2 points)
q1.pval <- format(round(pchisq(q1.curvetest$chisq, 1, lower.tail = FALSE), 3), nsmall = 3)
q1.pval

```


<div id="body">
**Type your answer here:** 

a)There are 41 observations of 4 variables. 

b)The median PFS is `r median.PFS` days and the median OS is `r median.OS` days.

c)There isn't evidence to support the hypothesis that there is a difference between the two groups. 

d)The p value is `r q1.pval`. If the curves were the same, the probability of getting the observed data is `r q1.pval`.

</div>

## Question 2 - 14 points

Of course, it is possible that confounders exist between different groups, especially in non-randomized datasets. Confounding may be controlled for as regression terms in the hazards model, but for a limited number of categories this may be investigated via stratification. Consider the `pharmacoSmoking` dataset in the `asaur` library. Remember the command `help("pharmacoSmoking")` will explain what the variables are.
<div id="exercise">
**Exercise**:     
a) How many observations and variables are in the dsataset? (2 points)
b) Is there a difference in time to relapse (TTR) according to the treatment received? (3 points)    
c) Next, stratify age by those 49 and younger and those 50 and older. How many patients are aged 50 and over? (3 points)
d) Repeat step b), but now include age as a potential confounder. Discuss if the age differences between the groups explain some or all of any observed treatment differences. (2 points)
</div>

```{r}
# insert R code here

q2.data <- pharmacoSmoking

head(pharmacoSmoking)


#a) How many observations and variables are in the dsataset? (2 points)
str(pharmacoSmoking)

#b) Is there a difference in time to relapse (TTR) according to the treatment received? (3 points)    
help("pharmacoSmoking")

q2.surv.object <- Surv(q2.data$ttr, q2.data$relapse)

#Combination is codded as 1 
#plot(survfit(q2.surv.object ~ q2.data$grp), col = c("blue", "red"), xlab = "Time to Relapse (Days)")
#legend(x = "topright", inset = c(0.05, 0.05), col = c("blue", "red"), lty = c(1, 1), legend = c("Combination", "Patch Only"))

q2.curvetest <- survdiff(q2.surv.object ~ q2.data$grp)
q2.curvetest
q2.b.pval <- pchisq(q2.curvetest$chisq, df = 1, lower.tail = F)


#c) Next, stratify age by those 49 and younger and those 50 and older. How many patients are aged 50 and over? (3 points)

#per group? No need to, there is a variable already
#q2.u50 <- nrow(filter(q2.data, q2.data$age < 50))
#q2.50ao <- nrow(filter(q2.data, q2.data$age >= 50))

nrow(q2.data)

#q2.data$ageReg <- as.factor(ifelse(q2.data$age < 50, "Under50", "50andOver"))

table(q2.data$ageReg, q2.data$grp)


#d) Repeat step b), but now include age as a potential confounder. Discuss if the age differences between the groups explain some or all of any observed treatment differences. (2 points)
q2.d.curvetest <- survdiff(q2.surv.object ~ q2.data$grp + strata(q2.data$ageGroup2))
q2.d.curvetest

q2.d.pval <- pchisq(q2.d.curvetest$chisq, df = 1, lower.tail = F)

#do we wat age as a dichotomous variable that we did in c? Or as a continuous varaible? Are continuous variables allowed?
#q2.cox <- coxph(q2.surv.object ~ q2.data$grp + q2.data$age)
#coxph(q2.surv.object ~ q2.data$grp)

#q2.cox.sum <- summary(q2.cox)
#patch.HR <- round(q2.cox.sum$coefficients[1,2], 2)
#age.HR <- round(q2.cox.sum$coefficients[2,2], 2)

#q2.cox.sum

#or is this just asking for surdiff on the two stratum?
#q2.data.under50 <- filter(q2.data, ageReg == "Under50")
#q2.data.under50
#q2.under50.obj <- Surv(q2.data.under50$ttr, q2.data.under50$relapse)
#survdiff(q2.under50.obj ~ q2.data.under50$grp)

#q2.data.50ao <- filter(q2.data, ageReg == "50andOver")
#q2.data.50ao
#q2.50ao.obj <- Surv(q2.data.50ao$ttr, q2.data.50ao$relapse)
#survdiff(q2.50ao.obj ~ q2.data.50ao$grp)


```

<div id="body">
**Type your answer here:** 

a)There are 125 observations of 14 variables. 

b)Yes, there is a difference. Using a null hypothesis of expected being no difference, the chi squared test of expected vs observed gives a p value of `r q2.b.pval`.

c)There are `r q2.u50` people aged 49 and under and `r q2.50ao` people aged 50 and over. They are split pretty evenly between the treatment groups. 

d)When stratifying by age and using a null hypothesis of expected being no difference between the treatment groups, the chi squared test of expected vs observed gives a p value of `r q2.d.pval` compared to 2d) unstratified p value of `r q2.b.pval`. This is evidence that the age difference between groups does not explain much (if any) of the observed differences between the treatment groups.

</div>

## Questions 3 - 10 points
Model selection can be a tricky subject. Let us begin with the simple situation of nested models. Using the `pharmacoSmoking` dataset, you will examine several nested Cox PH models. 
<div id="exercise">
**Exercise**: 
a) Create 3 Cox PH models with (1) no covariates, (2) age, and (3) age + employment (3 points)        
b) Report and interpret the log likleihoods for the 3 models (3 points)          
c) Does the addition of age statistically improve the null model? Report the p-value and interpret what it means. (2 points)     
d) If employment is included in the model, should age also be included? (2 points).  Hint; the `logLik` function may be useful.
</div>

```{r}
# insert R code here

#create the 3 models.....take out groups
q3.cox <- coxph(q2.surv.object ~ 1)
summary(q3.cox)

q3.cox.a <- coxph(q2.surv.object ~ q2.data$ageGroup4)
summary(q3.cox.a)

q3.cox.a.e <- coxph(q2.surv.object ~ q2.data$ageGroup4 + q2.data$employment)
summary(q3.cox.a.e)

#b) Report and interpret the log likleihoods for the 3 models (3 points)    
logLik(q3.cox)

q3.nocov.loglik <- q3.cox$loglik
q3.a.loglik <- q3.cox.a$loglik[2]
q3.a.e.loglik <- q3.cox.a.e$loglik[2]



#c) Does the addition of age statistically improve the null model? Report the p-value and interpret what it means. (2 points)   
q3.model.anova <- anova(q3.cox, q3.cox.a)
q3.c <- round(q3.model.anova$`P(>|Chi|)`[2], 4)
q3.c2 <- summary(q3.cox.a)
q3.c3 <- round(q3.c2$logtest[3], 4)



#d) If employment is included in the model, should age also be included? (2 points).  Hint; the `logLik` function may be useful.
q3.cox.e <- coxph(q2.surv.object ~ q2.data$employment)
q3.cox.e

#if simply using the log lik of the +emp model as the threshold, then look at the +age model. We are talking about neste models, so look at the age + emp model 
logLik(q3.cox.e)
logLik(q3.cox.a.e)
logLik(q3.cox.a)

#
q3.chi.avae <- round(1 - pchisq(as.numeric(-2*(logLik(q3.cox.a) - logLik(q3.cox.a.e))), df = 2), 3)

q3.chi.evae <- round(1 - pchisq(as.numeric(-2*(logLik(q3.cox.e) - logLik(q3.cox.a.e))), df = 1), 5)



#the age model gives a loglik closer to zero, ie: bigger, which means that if we are happy with emp on it's own being added, then we are happy with age on it's own being added. Obviously if we have an emp model and add age, that extra parameter automatically lowers the loglik


#just messing around here. This stuff gets used in next section
#number of parameters is all the coeffecients, in this case, intercept, group, and 2 additional works (3 work categories). So 1 + 1 + 2 = 4. Hold up! No intercept! So take 1 off of that shit. 1 + 2 = 3
AIC.e <- 2*3 - 2 *logLik(q3.cox.e)
AIC.e

#added age as a parameter, so 3 + 1 = 4
AIC.a.e <- 2*4 - 2 * logLik(q3.cox.a.e)
AIC.a.e


```

<div id="body">
**Type your answer here:** 

a)Summary of 3 models above. Created with age as a continuous variable in order to maximize data usage.

b)Model 1: `r q3.nocov.loglik`

Model 2: `r q3.a.loglik`

Model 3: `r q3.a.e.loglik`

There is no meaningful interpretation of the log-likelihoods on their own, but they can be used to compare models. They are related to the sum squares. Of the three models, Model 3 has the largest log likelihood and thus is relatively the best fit of the three models. Model 1 has the lowest, most negative log likelihood and is the worst of the three models. 

c)The anova comparing the models gives a p value of `r q3.c`. This means that there is a significant difference between the models. The likelihood ratio test gives p value of `r q3.c3`.

d)Age should be included in the model. The inclusion of employent in the ge model increases the log likelihood compared to the age only model ($\chi^2$ test comparing log likelihoods gives a p value of `r q3.chi.avae`, so reject the null hypothesis that they are the same). Comparing the log likelihoods of the employment only model to the age and employment model also shows that there is an improvement (p value of `r q3.chi.evae`). We can also just eyeball it and see that the log likelihood of the age and employment model (-379) is larger (less negative) than either the age only (-383) or employment only models (-385).....but it's like, a good thing to not rely on eyeballin' it and to go the extra mile and get a p value. 

</div>

## Questions 4 - 10 points
Suppose we now want to compare *non-nested* models. For example, the following models (1) age, (2) employment, (3) age + employment. While model 1 and 2 are each nested in model 3 and can be compared as above (i.e. models 1 vs 3, and models 2 vs 3), we can't use this technique to compare models 1 and 2. 
<div id="exercise">
**Exercise**:      
a) Calculate the AIC criteria ($\textit{AIC} = -2 * \ell(\widehat{\beta}) + 2 * k$) for each model (5 points)   
b) Discuss what the AIC criteria is, and its interpration in choosing the best of these 3 models (i.e. which model is best by AIC criteria and why?) (5 points)
</div>

```{r}
# insert R code here


#a) Calculate the AIC criteria ($\textit{AIC} = -2 * \ell(\widehat{\beta}) + 2 * k$) for each model (5 points)  

#number of parameters is all the coeffecients, in this case, no intercept, group, and 2 additional works (3 work categories). So 1 + 2 = 3
AIC.e <- round(2*2 - 2 *logLik(q3.cox.e)[1], 2)
AIC.e

#no intercept, just ageGroup4
AIC.a <- round(2*3 - 2 * logLik(q3.cox.a)[1], 2)
AIC.a



#employment and ageGroup4 as a parameter, so 2 + 3 = 3
AIC.a.e <- round(2*5 - 2 * logLik(q3.cox.a.e)[1], 2)
AIC.a.e
#confirm that it's the same. 
AIC(q3.cox.a.e, k = 2)

AIC.uni <- round(2*0 - 2* logLik(q3.cox)[1], 2)
AIC.uni
AIC(q3.cox, k = 2)

#b) Discuss what the AIC criteria is, and its interpration in choosing the best of these 3 models (i.e. which model is best by AIC criteria and why?) (5 points)



```

<div id="body">
**Type your answer here:** 
a) The AIC of the model without covariates is `r AIC.uni`. The AIC of the age only model is `r AIC.a`. The AIC of the employment only model is `r AIC.e`. The AIC of the age + employment model is `r AIC.a.e`. Note age as a categorical variable was used (4 categories). 

b) It is Akaikies Information Criterion that is meant to communicate the goodness of fit of a model. It has two important terms, one that is a measure of fit based on the sum squares of errors of prediction (SSE, aka residual sum of squares) and another that adds a penalty for each parameter added to the model. The penalty is meant to prevent over fitting. When comparing models, the model with the smallest AIC should be considered the best, though some argue that the AIC penalty for additional parameters is not strong enough and blindly chasing the lowest AIC will not always prevent overfitting (accding to Prof Lawrence). For this question, we have a lot of observations and events compared to the number of covariates we are looking at, so I'm not too worried about over fitting at the mo.

In comparing the three models based on the AIC criteria, the model with *age and employment* has the *lowest* AIC and is the *best* according to the AIC. AIC for no covariates: `r AIC.uni`, AIC for age: `r AIC.a`, AIC for employment `r AIC.e`, and AIC for age and employment: `r AIC.a.e`. Age as a continous variable was used. 

</div>

## Questions 5 - 10 points
Comparing models can be quite complex in the presence of multiple variables and the AIC is often automatically calculated using a stepwise procedure (use `R` function `step`)
<div id="exercise">
**Exercise**: .     
a) Start with the complete model of predictors, without interaction terms, in the `pharmacoSmoking` data set and then use `step` to arrive at the most parsimonious model. What are the variables in the final model? Use `?step` for syntax. (5 points)    
b) What are the odds ratios for the age variable categories, based on the final model you selected. (5 points)
</div>

```{r}
# insert R code here
?step
head(q2.data)
str(q2.data)

q5.full.model <- coxph(data = q2.data, formula = q2.surv.object ~ . - id - ttr - relapse - ageGroup2 - ageGroup4 - ageReg)
summary(q5.full.model)

q5.step <- step(q5.full.model, direction = "backward")
summary(q5.step)

#repeat with dichotomous age 
q5.full.model.agedic <- coxph(data = q2.data, formula = q2.surv.object ~ . - id - ttr - relapse - ageGroup2 - ageGroup4 - age)
summary(q5.full.model.agedic)


q5.step.agedic <- step(q5.full.model.agedic, direction = "backward")
summary(q5.step.agedic)

#repeat with 4 cat age
q5.full.model.agecat <- coxph(data = q2.data, formula = q2.surv.object ~ . - id - ttr - relapse - ageGroup2 - age - ageReg)
summary(q5.full.model.agecat)


q5.step.agecat <- step(q5.full.model.agecat, direction = "backward")
summary(q5.step.agecat)

```


<div id="body">
**Type your answer here:** 

a)The most parsimonious model includes the treatment group, age (categorical with 4 cats), and employment variables. We also repeated the process using the dichotomous and continuous age and came to the same model.

b)If age has 4 categories, the hazard ratio for the 35-49 age variable is 0.89 (95% CI of 0.48, 1.68) and suggests 35-49 year olds have a 11% lower hazard of relapse compared to 21-34 year olds, but the HR 95% CI is very wide and crosses the null in a big way and is thus inconclusive. The hazard ratio for the 50-64 age variable is 0.36 (95% CI of 0.18, 0.73) and suggests 50-64 year olds have a 64% lower hazard of relapse compared to 21-34 year olds with the 95%CI being well below the null. The hazard ratio for the 65+ age variable is 0.49 (95% CI of 0.18, 1.32) and suggests 65+ year olds have a 51% lower hazard of relapse compared to 21-34 year olds, though the HR crosses the null and is thus inconclusive, but may be interesting because a large part of the 95% CI is below 1. These hazard ratios are all in comparison to the reference category of 21-34 years old. 

</div>

## Question 6 - 5 points
In the previous exercise, we see that the effect of age group appears variable. The natural question is whether this is a linear or non-linear relationship. In cases of non-linear relationships, one can use splines to model data. Splines are flexible "piecewise polynomials", meaning that they are distinct curves that attach smoothly (as opposed to disjointedly) at specific points called "knots".

The following code will fit a smoothing spline to this data and graph the results; 

`modelS4.coxph <- coxph(Surv(ttr, relapse) ~ grp + employment + pspline(age, df=4) )`     
`termplot(modelS4.coxph, se=T, terms=3, ylabs="Log hazard")` 

<div id="exercise">
**Exercise**:     
Based on this information, can we establish that the relationship has departed from linearity? Consider curve shape and confidence intervals, and anything else you find relevant. (5 points)
</div>

```{r}
# insert R code here
modelS4.coxph <- coxph(Surv(ttr, relapse) ~ grp + employment + pspline(age, df=4), data = q2.data)
modelS4.coxph

#not working for me
termplot(modelS4.coxph, se=T, terms=3, ylabs="Log hazard")

hist(q2.data$age, main = "Histogram of Age", xlab = "Age")
```

<div id="body">
**Type your answer here:** 

After age 60 the 95% confidence intervals get really large. Under age 30 also seems to have fairly large 95% CI. If we look at the histogram of age, we see that the age data looks fairly normal, but there is a bit of a long right tail. We have a few data points above age 70 that could be influencing the stability of our estimates on either end of out regression. At first glance, the log(hazard) vs age plot looks non-linear, but the large CIs at either end don't rule out linearity. Notice how you can trace a nice, straight, downward, linear line within the 95% CI. We cannot establish that the relationship has departed from linearity. 

</div>

## Question 7 - 5 points
Model goodness-of-fit can also be assessed in survival analysis, in an analogous manner to linear regression, i.e. by examining residuals. You will need to use `residuals` a generic function to extract model residuals that may be applied to survival objects. Details can be found by `?residual.coxph`. You will need to download the function `smoothSEcurve` from MyCourses and then source this into your file in order to plot the confidence intervals.

<div id="exercise">
**Exercise**:  Produce a separate residual plot for each covariate, then interpret each plot with respect to model goodness-of-fit (5 points).
</div>

```{r}
# insert R code here

smoothSEcurve <- function(yy, xx) {
  # use after a call to "plot"   
  # fit a lowess curve and 95% confidence interval curve
  
  # make list of x values
  xx.list <- min(xx) + ((0:100)/100)*(max(xx) - min(xx)) 

  # Then fit loess function through the points (xx, yy)
  #   at the listed values
  yy.xx <- predict(loess(yy ~ xx), se=T, 
     newdata=data.frame(xx=xx.list))

  lines(yy.xx$fit ~ xx.list, lwd=2)
  lines(yy.xx$fit - 
       qt(0.975, yy.xx$df)*yy.xx$se.fit ~ xx.list, lty=2)
  lines(yy.xx$fit + 
       qt(0.975, yy.xx$df)*yy.xx$se.fit ~ xx.list, lty=2)
  }


q7.resids <- residuals(modelS4.coxph, type = "martingale")
par(mfrow=c(2, 2))
plot(q7.resids ~ q2.data$age)
smoothSEcurve(q7.resids, q2.data$age)
abline(a = 0, b = 0, col = "blue")
plot(q7.resids ~ log(q2.data$age))
smoothSEcurve(q7.resids, log(q2.data$age))
abline(a = 0, b = 0, col = "blue")
plot(q7.resids ~ q2.data$employment)
abline(a = 0, b = 0, col = "blue")
plot(q7.resids ~ q2.data$grp)
abline(a = 0, b = 0, col = "blue")
par(mfrow=c(1, 1))




```


<div id="body">
**Type your answer here:** 

Horizontal blue lines were added to the plots to help with eyeballin' the graphs. We took a good loooooong look at these here graphs and we're happy with them. The residuals look evenly distributed accross each of the covariates: the 95% CI smooth lines have a nice, horizontal area accross the age range and the boxplots are fairly even and centered on zero. Could they be more better? Sure, but I'm not going to lose sleep on it or waste one of my three wishes flattening them out. Hear ye hear ye, we declare these to be linear enough!

</div>

## Question 8 - 10 points
Occasionally you want to be sure that individual measurements are not unduly influencing the model (for example if data was incorrectly entered). One approach to this is the deletion of individual values and assessing the impact on the model coefficients. This can be done by adding `type=dfbeta` to the `residuals` function. More information can be found by typing `?dfbeta`. 

<div id="exercise">
**Exercise**:     
a) Plot the age coefficient for each data as a function of the deleted observation. Use the covariates from the final model you selected in question 5 (5 points)         
b) Discuss whether this graph suggests that any values are exerting an undue influence on the model. Hint: consider the magnitude of the coefficient change  (5 points)  
</div>

```{r}
# insert R code here
?dfbeta()

q8.resid <- residuals(modelS4.coxph, type = "dfbeta")

num.obs <- length(q2.data$ttr)
index.obs <- 1:num.obs
plot(q8.resid[,4] ~index.obs, type="h", xlab="Observations", ylab="Change in coefficients", main = "Change in Age Coefficient with Removal of Observations")
abline(h=0)

q8.resid[80:90, 4]

which(q8.resid[,4] == min(q8.resid[,4]))

#you know how sometimes in regression modelling, you fit the model to your data, but what if you have some outliers, how much do they influence it. Play around with the rows, removing each one and seeing what happens to betas. 
```

<div id="body">
**Type your answer here:** 

a)Plotted above.

b)Most of the values don't have a huge influence on the model, however there is one value whose influence on the model coefficients is nearly 3 times greater than the next "biggest influencer" (ie: if that observation is removed, the change in coefficient is 3 times greater than if the next biggest influencer observation is removed). It's like Huda Kattan left Instagram and decided to be in this model. It's the 87th row of the data frame. Apart from that one, there are about half a dozen other middle level influencers. They do alright, but they ain't having lunch with any of the Kardashians. 

</div>

## Question 9 - 9 points
Proportional hazards is a key assumption of the Cox model and this can be checked, in a somewhat crude manner, by a log-log plot. Use the `pancreatic2` data, a slightly modified from `pancreatic` data set in Question 1 from the `asaur` package.

<div id="exercise">
**Exercise**:     
a) Model PFS survival separately as a function of `stage`. Plot time axis in months. (3 points)    
b) Plot the complementary log-log survival against log time for each individual cancer stage. Please provide a legend to indicate which curve is which stage. (3 points).      
c) Interpret the graph as to whether you think the proportional hazards assumption is valid (3 points).
</div>

```{r}
# insert R code here

q9.data <- pancreatic2

head(q9.data)
str(q9.data)

mean(q9.data$status)

q9.data$pfsmo <- q9.data$pfs/30.5

q9.surv.obj <- Surv(q9.data$pfsmo)

q9.cox <- coxph(q9.surv.obj ~ strata(q9.data$stage))


#this is how we dooooo iiiiit (Montel Jordan)
head(q9.data)

q9.LA.fit <- survfit(Surv(q9.data$pfsmo) ~ q9.data$stage, subset = {q9.data$stage == "LA"})
q9.M.fit <- survfit(Surv(q9.data$pfsmo) ~ q9.data$stage, subset = {q9.data$stage == "M"})

LA.t <- q9.LA.fit$time
LA.surv <- q9.LA.fit$surv

M.t <- q9.M.fit$time
M.surv <- q9.M.fit$surv

comp.LA <- log(-log(LA.surv))
comp.M <- log(-log(M.surv))
log.t.LA <- log(LA.t)
log.t.M <- log(M.t)

LA.df <- data.frame(comp = comp.LA, log.t = log.t.LA, Stage = rep("Locally Advanced", length(comp.LA)))
M.df <- data.frame(comp = comp.M, log.t = log.t.M, Stage = rep("Mestatic", length(comp.M)))

q9.plot.data <- as.data.frame(rbind(LA.df, M.df))

ggplot(data = q9.plot.data, aes(x = log.t, y = comp, col = Stage)) + geom_step() + labs(title = "Checking Proportional Hazards Assumption", y = "log(-log(survival)", x ="log(t)")

ggplot(data = q9.plot.data, aes(x = log.t, y = -comp, col = Stage)) + geom_step() + labs(title = "Checking Proportional Hazards Assumption", y = "-log(-log(survival)", x ="log(t)")


```


<div id="body">
**Type your answer here:** 

a)

b)

c)The lines are not parrallel so the assumption that the hazards are proportional is violated.  It might have been okay if they were close to parrallel, but they full on cross each other. Just like in ghostbusters, crossing streams is a no-no. This suggests that the baseline hazards do not cancel each other out. Keep in mind we have a pretty small sample size (only 8 LAs!)

</div>

## Question 10  - 6 points
Schoenfeld residuals provide another useful way to evaluate the proportional hazards assumption. In `R` these residuals may be calculated by applying the `cox.zph` function to the `R` survival object. 

<div id="exercise">
**Exercise**:      
a) Plot these residaul for the pancreatic model from Question 9 (3 points)
b) What is the associated p-value and what is your interpretation of the proportional hazards assumption using this technique? (3 points)
</div>

```{r}
# insert R code here

q10.cox <- coxph(q9.surv.obj ~ q9.data$stage)
q.10.z <- cox.zph(q10.cox, transform = "km")
q.10.pval <- round(q.10.z$table[3], 3)

library(survminer)
ggcoxzph(q.10.z)



```


<div id="body">
**Type your answer here:** \

b)The p value is `r q.10.pval`, which is evidence that there is a relatioship between the residuals and time. There also appears to be a pattern between residuals and time on the plot so this is further evidence that the proportional hazard assumption is violated. We could trace a horizontal line through the 95% CI, but that would be tough, there seems to be a downward trend. To address this, we may need to look for an interaction term of one of the covariates with time or stratify by some other covariate. 

</div>
